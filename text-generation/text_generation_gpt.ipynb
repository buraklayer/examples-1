{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text generation gpt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3297672a41a4d04b62a0d20015219ee": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_a9be955548284e969365231cf5839dbf",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "✅  gpt-2                \u001b[38;2;52;211;153m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[38;2;52;211;153m        done         \u001b[0m \u001b[39m[ \u001b[0m\u001b[33m0:00:26\u001b[0m\u001b[39m ]\u001b[0m \n    \u001b]8;id=170856;https://development.layer.co/layer/sandbox-config/models/gpt-2\u001b\\\u001b[4;38;2;161;161;169mhttps://development.layer.co/layer/sandbox-config/models/gpt-2\u001b[0m\u001b]8;;\u001b\\                \n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅  gpt-2                <span style=\"color: #34d399; text-decoration-color: #34d399\">━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #34d399; text-decoration-color: #34d399\">        done         </span> <span style=\"color: #000000; text-decoration-color: #000000\">[ </span><span style=\"color: #808000; text-decoration-color: #808000\">0:00:26</span><span style=\"color: #000000; text-decoration-color: #000000\"> ]</span> \n    <a href=\"https://development.layer.co/layer/sandbox-config/models/gpt-2\"><span style=\"color: #a1a1a9; text-decoration-color: #a1a1a9; text-decoration: underline\">https://development.layer.co/layer/sandbox-config/models/gpt-2</span></a>                \n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "a9be955548284e969365231cf5839dbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad7874f027d440dfa4b68650989b3f56": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_6fe623ffad9a43bd8513a04484885fea",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "✅  tokenizer-tester     \u001b[38;2;52;211;153m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[38;2;52;211;153m        done         \u001b[0m \u001b[39m[ \u001b[0m\u001b[33m0:00:17\u001b[0m\u001b[39m ]\u001b[0m \n    \u001b]8;id=47107;https://development.layer.co/layer/sandbox-config/models/tokenizer-tester\u001b\\\u001b[4;38;2;161;161;169mhttps://development.layer.co/layer/sandbox-config/models/tokenizer-tester\u001b[0m\u001b]8;;\u001b\\     \n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅  tokenizer-tester     <span style=\"color: #34d399; text-decoration-color: #34d399\">━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #34d399; text-decoration-color: #34d399\">        done         </span> <span style=\"color: #000000; text-decoration-color: #000000\">[ </span><span style=\"color: #808000; text-decoration-color: #808000\">0:00:17</span><span style=\"color: #000000; text-decoration-color: #000000\"> ]</span> \n    <a href=\"https://development.layer.co/layer/sandbox-config/models/tokenizer-tester\"><span style=\"color: #a1a1a9; text-decoration-color: #a1a1a9; text-decoration: underline\">https://development.layer.co/layer/sandbox-config/models/tokenizer-tester</span></a>     \n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "6fe623ffad9a43bd8513a04484885fea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFaRc3Ozxzoy",
        "outputId": "7a2e8b3b-da3c-4c91-8c43-86b6dbae0916",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 29.2 MB 1.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 159 kB 41.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 34.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 36.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 51.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 3.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 380 kB 58.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 50.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 39.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 96 kB 3.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 16.5 MB 238 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 47.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 51.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 35.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 256 kB 52.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 54 kB/s \n",
            "\u001b[K     |████████████████████████████████| 40 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 26.7 MB 1.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 50.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 49.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 66.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 88 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 39.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 50.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 48.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 263 kB 48.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 929 kB 44.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 38.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 48.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 38.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 210 kB 73.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 146 kB 72.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 66.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 5.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 291 kB 50.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 78 kB 5.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 95 kB 3.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 224 kB 60.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 60.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 51.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 42.8 MB/s \n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'grpcio' candidate (version 1.45.0 at https://files.pythonhosted.org/packages/b8/ab/c7abc950e222c4cab5f7fd92107f3b09553061f673f1a670e6569105f584/grpcio-1.45.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl#sha256=cc135b77f384a84bac67a37947886986be136356446338d64160a30c85f20c6d (from https://pypi.org/simple/grpcio/) (requires-python:>=3.6))\n",
            "Reason for being yanked: Segfaults\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for validate-email (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for polling (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "nbclient 0.5.13 requires jupyter-client>=6.1.5, but you have jupyter-client 5.3.5 which is incompatible.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.28 which is incompatible.\n",
            "ipython 5.5.0 requires prompt-toolkit<2.0.0,>=1.0.4, but you have prompt-toolkit 3.0.28 which is incompatible.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install layer-sdk --upgrade -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import layer\n",
        "layer.login(\"https://development.layer.co\")\n",
        "layer.init(\"sandbox-config\")"
      ],
      "metadata": {
        "id": "1QIRRAeMyCf-",
        "outputId": "289e1435-0966-46b8-9e08-e3bd6e02d6bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Project(name='sandbox-config', raw_datasets=[], derived_datasets=[], featuresets=[], models=[], path=PosixPath('.'), project_files_hash='', readme='', organization=Organization(id=UUID('d7325da3-0646-4fa6-855d-8d19eece8b79'), name='layer'), _id=UUID('827a4a74-cd10-4924-9a8e-ecbbe61078ad'), functions=[])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from layer.decorators import model,pip_requirements,fabric"
      ],
      "metadata": {
        "id": "auElcEE5-wPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@pip_requirements(packages=[\"transformers\",\"sentencepiece\"])\n",
        "@fabric(\"f-medium\")\n",
        "@model(\"gpt-2\")\n",
        "def train():\n",
        "  from transformers import TFGPT2LMHeadModel, GPT2Config\n",
        "  config = GPT2Config.from_pretrained(\"gpt2\")\n",
        "  model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\",config=config)\n",
        "  return model"
      ],
      "metadata": {
        "id": "HrF5RIGy3VXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer.run([train])"
      ],
      "metadata": {
        "id": "5UufssLl_i3W",
        "outputId": "eadedf1e-8647-467a-9ffd-52dafd510571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70,
          "referenced_widgets": [
            "e3297672a41a4d04b62a0d20015219ee",
            "a9be955548284e969365231cf5839dbf"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3297672a41a4d04b62a0d20015219ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Run(project_name='sandbox-config')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2 = layer.get_model('gpt-2').get_train()\n",
        "gpt2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp75Wi_93eK8",
        "outputId": "44c4ab42-6312-4bc6-d7e3-275c50159e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at /tmp/tmpr5upb0d4/model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel at 0x7fca275c3090>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@pip_requirements(packages=[\"transformers\",\"sentencepiece\"])\n",
        "@fabric(\"f-medium\")\n",
        "@model(name=\"tokenizer-tester\")\n",
        "def download_tokenizer():\n",
        "  from transformers import GPT2Tokenizer\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "K91J97AyGEVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the project on Layer Infra\n",
        "layer.run([download_tokenizer])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70,
          "referenced_widgets": [
            "ad7874f027d440dfa4b68650989b3f56",
            "6fe623ffad9a43bd8513a04484885fea"
          ]
        },
        "id": "lvg-tFK7GIMp",
        "outputId": "701956bd-3849-46dd-bcbf-436fd33fe695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad7874f027d440dfa4b68650989b3f56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Run(project_name='sandbox-config')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = layer.get_model('tokenizer-tester').get_train()\n",
        "input_sequence = \"The farmer planted a lot of crops\"\n",
        "\n",
        "# encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n",
        "\n",
        "# generate text until the output length (which includes the context length) reaches 50\n",
        "greedy_output = gpt2.generate(input_ids,max_length=100)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens = True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5lLNjfm3iaf",
        "outputId": "42f5cc94-faf1-45ba-8a27-ce4249497cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "The farmer planted a lot of crops in the area, but the farmers didn't want to be seen as a threat to the community.\n",
            "\n",
            "\"We're not going to be able to do anything about it,\" said the farmer.\n",
            "\n",
            "The farmer said he was concerned about the impact of the drought on the community.\n",
            "\n",
            "\"We're not going to be able to do anything about it,\" he said.\n",
            "\n",
            "The farmer said he was concerned about the impact of the drought on the\n"
          ]
        }
      ]
    }
  ]
}